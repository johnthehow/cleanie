# ç¨‹åºæ€è·¯è§corprepæ€»ä½“è§„åˆ’_20211024171633.drawio
# é™„ä»¶: æ¬§æ´²è¯­è¨€ç‰¹æ®Šå­—æ¯è¡¨_20211022170741.zip
# ç”¨ä¾‹: cleannie.pipeline(r'X:/test.txt', r'X:/')
# 		è¾“å…¥: æœªç»å¤„ç†çš„å¹³ç™½æ–‡æœ¬
# 		è¾“å‡º: å¤„ç†åçš„å¹³ç™½æ–‡æœ¬å’Œç»Ÿè®¡
import re
import string
import pickle
from pathlib import Path
import warnings
import bs4
warnings.filterwarnings("ignore")

BASIC_LATIN_LOWER = 'abcdefghijklmnopqrstuvwxyz'
BASIC_LATIN_UPPER = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'
BASIC_LATIN_ABC = BASIC_LATIN_LOWER + BASIC_LATIN_UPPER
LATIN_SUP = 'Ã€ÃÃ‚ÃƒÃ„Ã…Ã†Ã‡ÃˆÃ‰ÃŠÃ‹ÃŒÃÃÃÃÃ‘Ã’Ã“Ã”Ã•Ã–Ã˜Ã™ÃšÃ›ÃœÃÃÃŸÃ Ã¡Ã¢Ã£Ã¤Ã¥Ã¦Ã§Ã¨Ã©ÃªÃ«Ã¬Ã­Ã®Ã¯Ã°Ã±Ã²Ã³Ã´ÃµÃ¶Ã¸Ã¹ÃºÃ»Ã¼Ã½Ã¾Ã¿'
LATIN_EXT_A = 'Ä€ÄÄ‚ÄƒÄ„Ä…Ä†Ä‡ÄˆÄ‰ÄŠÄ‹ÄŒÄÄÄÄÄ‘Ä’Ä“Ä”Ä•Ä–Ä—Ä˜Ä™ÄšÄ›ÄœÄÄÄŸÄ Ä¡Ä¢Ä£Ä¤Ä¥Ä¦Ä§Ä¨Ä©ÄªÄ«Ä¬Ä­Ä®Ä¯Ä°Ä±Ä²Ä³Ä´ÄµÄ¶Ä·Ä¸Ä¹ÄºÄ»Ä¼Ä½Ä¾Ä¿Å€ÅÅ‚ÅƒÅ„Å…Å†Å‡ÅˆÅŠÅ‹ÅŒÅÅÅÅÅ‘Å’Å“Å”Å•Å–Å—Å˜Å™ÅšÅ›ÅœÅÅÅŸÅ Å¡Å¢Å£Å¤Å¥Å¦Å§Å¨Å©ÅªÅ«Å¬Å­Å®Å¯Å°Å±Å²Å³Å´ÅµÅ¶Å·Å¸Å¹ÅºÅ»Å¼Å½Å¾Å¿'
LATIN_ABC = BASIC_LATIN_ABC + LATIN_SUP + LATIN_EXT_A

EN_SPECIAL_ABC_LOWER = ''
EN_SPECIAL_ABC_UPPER = ''
LANG = 'EN'
PRINT_TITLE = 'PIPELINE_EN'
EN_SPECIAL_ABC = EN_SPECIAL_ABC_LOWER + EN_SPECIAL_ABC_UPPER
END_PUNCT = '[.!?]'

LEGAL_ABC = LATIN_ABC + LATIN_SUP +LATIN_EXT_A + EN_SPECIAL_ABC + DIACRITICS
LEGAL_DIGIT = '0123456789'
VOCAL_CHAR = LEGAL_ABC + LEGAL_DIGIT
LEGAL_PUNCT = string.punctuation + 'Â°' + '$Â£â‚¬Â¥â‚¹'
LEGAL_CHAR = LEGAL_ABC + LEGAL_PUNCT + LEGAL_DIGIT

CUSTOM_REPLACE = dict()
with open('custom_replace.txt',mode='r',encoding='utf-8') as file:
	cr_lines = file.readlines()
	cr_lines = [i[:-1] for i in cr_lines] #å»é™¤æœ«å°¾çš„æ¢è¡Œç¬¦\n
	cr_lines = [i.split(sep='\t') for i in cr_lines]
	for i in cr_lines:
		CUSTOM_REPLACE[i[0]] = i[1]



# åœ¨æ•´ä¸ªæ–‡æœ¬ä¸­æ›¿æ¢ä¹±ç 
def replace_gibberish_doc(doc):
	hex_map = CUSTOM_REPLACE
	hex_map_keys = list(hex_map.keys()) 
	for key in hex_map_keys:
		doc = doc.replace(key,hex_map[key])
	return doc

# åœ¨æ¯å¥è¯ä¸­æ›¿æ¢ä¹±ç 
def replace_gibberish_strlines(strlines):
	proc_lines = []
	hex_map = CUSTOM_REPLACE
	hex_map_keys = list(hex_map.keys()) 
	for line in strlines:
		for key in hex_map_keys:
			line = line.replace(key,hex_map[key])
		proc_lines.append(line)
	return proc_lines


# åˆ é™¤å¤šä½™ç©ºæ ¼å’Œéç©ºæ ¼é—´éš”ç¬¦ä¸ºç©ºæ ¼, å¿…é¡»åœ¨åˆ†è¡Œå‰ä½¿ç”¨
def remove_extra_space(strlines):
	proc_lines = []
	for line in strlines:
		proc_line = re.sub(' {2,}',' ',line)
		proc_lines.append(proc_line)
	return proc_lines


# è®°å½•ä¸åˆæ³•çš„token: å³å«æœ‰æœªå®šä¹‰åœ¨åˆæ³•å­—ç¬¦ä¸­çš„å­—ç¬¦çš„token
def log_illegal_token(tklines):
	rec = []
	for line in tklines:
		for token in line:
			illegal_cnt = 0
			for char in token:
				if char not in LEGAL_CHAR:
					illegal_cnt += 1
			if illegal_cnt != 0:
				rec.append(token)
	res = '\n'.join(rec)
	return res


# æ›¿æ¢htmlå­—ç¬¦å®ä½“, å¿…é¡»åœ¨åˆ†è¡Œåä½¿ç”¨, åˆ†è¡Œå‰ä½¿ç”¨ä¼šä¸¢å¤±éƒ¨åˆ†æ¢è¡Œç¬¦
def replace_html_char_entity(strlines):
	proc_lines = []
	for line in strlines:
		proc_lines.append(bs4.BeautifulSoup(line).text)
	return proc_lines


# åˆ†è¡Œåä½¿ç”¨, åˆ é™¤ç©ºè¡Œ
def remove_empty_strlines(strlines):
	res = []
	empty_line_cnt = 0
	for line in strlines:
		if len(line) != 0:
			res.append(line)
		else:
			empty_line_cnt += 1
	return res


# æ‰€æœ‰æ¸…ç†å®Œæˆå, åˆ é™¤ç©ºè¡Œ
def remove_empty_tklines(tklines):
	res = []
	empty_line_cnt = 0
	for line in tklines:
		if len(line) != 0:
			res.append(line)
		else:
			empty_line_cnt += 1
	print(f'{PRINT_TITLE}: {empty_line_cnt} Empty Lines Removed')
	return res

# åˆ¤æ–­ä¸€ä¸ªtokenæ˜¯å¦å«æœ‰éæ³•å­—ç¬¦
def is_contain_illegal_char(token):
	res = False
	for char in token:
		if char not in LEGAL_CHAR:
			res = True
	return res

# åˆ¤æ–­ä¸€ä¸ªtokenæ˜¯å¦å«æœ‰ä¸”åªå«æœ‰å­—æ¯
def is_only_legalabc(token):
	match_cnt = 0
	for char in token:
		if char in LEGAL_ABC:
			match_cnt += 1
	if match_cnt == len(token) and len(token) != 0:
		res = True
	else:
		res = False
	return res


def is_only_numeric(token):
	match_cnt = 0
	for char in token:
		if char in LEGAL_DIGIT:
			match_cnt += 1
	if match_cnt == len(token) and len(token) != 0:
		res = True
	else:
		res = False
	return res


# åˆ¤æ–­ä¸€ä¸ªtokenæ˜¯å¦ å«æœ‰ä¸”åªå«æœ‰éå‘å£°å­—ç¬¦
def is_only_nonvocal(token):
	match_cnt = 0
	for char in token:
		if char not in LEGAL_ABC and char not in LEGAL_DIGIT:
			match_cnt += 1
	if match_cnt == len(token) and len(token) != 0:
		res = True
	else:
		res = False
	return res


# åˆ¤æ–­tokenæ˜¯å¦ä¸ºå•è´¨å­—ç¬¦ä¸², ä¾èµ–å‡½æ•°is_only_en_abc(), is_only_nonvocal()
def is_homo(token):
	return is_only_legalabc(token) or is_only_numeric(token) or is_only_nonvocal(token)  and len(token) != 0


# åˆ¤æ–­tokenæ˜¯å¦ æ˜¯ä¸”ä»…æ˜¯ å­—æ¯+ç¬¦å· çš„æ··åˆ
def is_mix_legalabc_nonvocal(token):
	abc_cnt = 0
	nonvocal_cnt = 0
	for char in token:
		if char in LEGAL_ABC:
			abc_cnt += 1
		elif char not in VOCAL_CHAR:
			nonvocal_cnt += 1
	if abc_cnt + nonvocal_cnt == len(token) and abc_cnt*nonvocal_cnt and len(token) != 0:
		res = True
	else:
		res = False
	return res


# åˆ¤æ–­ä¸€ä¸ªtokenæ˜¯å¦æ˜¯ å­—æ¯å’Œæ•°å­—çš„æ··åˆ
def is_mix_legalabc_num(token):
	abc_cnt = 0
	num_cnt = 0
	for char in token:
		if char in LEGAL_ABC:
			abc_cnt += 1
		elif char in LEGAL_DIGIT:
			num_cnt += 1
	if abc_cnt + num_cnt == len(token) and abc_cnt*num_cnt and len(token) !=0 :
		res = True
	else:
		res = False
	return res

# åˆ¤æ–­ä¸€ä¸ªtokenæ˜¯å¦æ˜¯ä¸”ä»…æ˜¯æ•°å­—å’Œç¬¦å·çš„æ··åˆ
def is_mix_num_nonvocal(token):
	num_cnt = 0
	nonvocal_cnt = 0
	for char in token:
		if char in LEGAL_DIGIT:
			num_cnt += 1
		elif char not in LEGAL_ABC:
			nonvocal_cnt += 1
	if num_cnt + nonvocal_cnt == len(token) and num_cnt*nonvocal_cnt and len(token) !=0:
		res = True
	else:
		res = False
	return res


# åˆ¤æ–­ä¸€ä¸ªtokenæ˜¯å¦æ˜¯ä¸”ä»…æ˜¯ æ•°å­—-ç¬¦å·-å­—æ¯ çš„æ··åˆ
def is_mix_legalabc_num_nonvocal(token):
	res = False
	if is_homo(token) == False:
		if is_mix_legalabc_num(token) == False:
			if is_mix_legalabc_nonvocal(token) == False:
				if is_mix_num_nonvocal(token) == False:
					if len(token) != 0:
						res = True
	return res


# åˆ¤æ–­ä¸€ä¸ªtokenæ˜¯å¦å«æœ‰ä¸”ä»…å«æœ‰ä¸€æ®µè¿ç»­çš„å­—æ¯ä¸²
def is_contain_onlyone_legalabc_seq(token):
	char_cnt = 0
	abc_cnt = 0
	abc_idxs = []
	for char in token:
		char_cnt += 1
		if char in LEGAL_ABC:
			abc_cnt += 1
			abc_idxs.append(char_cnt)
	res = all(a+1==b for a, b in zip(abc_idxs, abc_idxs[1:])) and abc_cnt !=0
	return res

# å«æœ‰ä¸”ä»…å«æœ‰ä¸€æ®µè¿ç»­çš„å­—æ¯ä¸²çš„tokenä¸­æ‰€æœ‰çš„å­—æ¯å¹¶ç²˜åˆä¸ºä¸€ä¸ªtoken
# ä¾èµ– is_contain_onlyone_legalabc_seq
def keep_only_legalabc_seq(token):
	if is_contain_onlyone_legalabc_seq(token):
		proc_token = ''
		for char in token:
			if char in LEGAL_ABC:
				proc_token += char
	else:
		print('keep_only_legalabc_seq: contain more than one continuous abc sequence')
	return proc_token


# åˆ¤æ–­æ˜¯å¦æœ‰å½¢å¦‚state-of-the-art, vis-a-viså½¢å¼çš„å•è¯
def is_contain_sota(token):
	regex = re.compile(f'[{LEGAL_ABC}]+(-[{LEGAL_ABC}]+)+')
	if re.search(regex,token) != None:
		res = True
	else:
		res = False
	return res

def keep_only_sota(token):
	if is_contain_sota(token):
		regex = re.compile(f'[{LEGAL_ABC}]+(-[{LEGAL_ABC}]+)+')
		match = re.search(regex,token)
		res = match.group()
	else:
		print('keep_only_sota: trying to extract sota while don\'t have one')
	return res

# åˆ¤æ–­æ˜¯å¦æ˜¯ u.s.a. æˆ– u.s.a æˆ– a.m. æˆ– p.må½¢å¼
def is_usa(token):
	regex = re.compile(f'^[{LEGAL_ABC}](\.[{LEGAL_ABC}])+\.?$')
	if re.search(regex,token) != None:
		res = True
	else:
		res = False
	return res

# åˆ¤æ–­æ˜¯å¦æ˜¯ ^å•è¯,å•è¯$ å½¢å¼çš„token
def is_abc_comma_abc(token):
	regex = re.compile(f'^([{LEGAL_ABC}]{{2,}})(,)([{LEGAL_ABC}]{{2,}})$')
	if re.search(regex,token) != None:
		res = True
	else:
		res = False
	return res


# æ‹†åˆ†é€—å·åˆ†éš”çš„å•è¯
def split_abc_comma_abc(token):
	if is_abc_comma_abc(token):
		res = []
		regex = re.compile(f'^([{LEGAL_ABC}]{{2,}})(,)([{LEGAL_ABC}]{{2,}})$')
		match = re.search(regex,token)
		res.append(match.group(1))
		res.append(match.group(3))
	else:
		print('split_abc_comma_abc: trying to split abc,abc while is not one')
	return res

# åˆ¤æ–­æ˜¯å¦æ˜¯.!?åˆ†éš”çš„å•è¯
def is_abc_endpunct_abc(token):
	regex = re.compile(f'^([{LEGAL_ABC}]{{2,}})({END_PUNCT})([{LEGAL_ABC}]{{2,}})$')
	if re.search(regex,token) != None:
		res = True
	else:
		res = False
	return res

# æ‹†åˆ†.!?åˆ†éš”çš„å•è¯
def split_abc_endpunct_abc(token):
	if is_abc_endpunct_abc(token):
		res = []
		regex = re.compile(f'^([{LEGAL_ABC}]{{2,}})({END_PUNCT})([{LEGAL_ABC}]{{2,}})$')
		match = re.search(regex,token)
		res.append(match.group(1))
		res.append(match.group(3))
	else:
		print('split_abc_endpunct_abc: trying to split abc.!?abc while is not one')
	return res

# æ˜¯å¦åŒ…å«'å·ç¼©çº¦å¼
def is_contain_contract(token):
	res = False
	regex = re.compile(f'[{LEGAL_ABC}]+\'[{LEGAL_ABC}]+')
	if re.search(regex,token) != None:
		if len(re.findall('\'',token)) == 1:
			res = True
	return res

# ä¾èµ– is_contain_contract
def keep_only_contract(token):
	regex = re.compile(f'[{LEGAL_ABC}]+\'[{LEGAL_ABC}]+')
	if is_contain_contract(token) == True:
		match = re.search(regex,token).group()
	return match

# æ¸…é™¤tokenä¸¤ç«¯çš„éå‘å£°ç¬¦å·
def strip_nonvocal_mix_nonvocal(token):
	origin = token
	if token[0] not in VOCAL_CHAR:
		token = token[1:]
	else:
		pass
	if token[-1] not in VOCAL_CHAR:
		token = token[:-1]
	else:
		pass
	if token == origin:
		pass
	else:
		token = strip_nonvocal_mix_nonvocal(token)
	return token

# ä¾èµ–: æ— ä¾èµ–å‡½æ•°
# å„ç§å¼•å·å¤§å…¨è§https://unicode-table.com/en/sets/quotation-marks/
def replace_nonascii_punct(strlines):
	#å‰è¾¹: é”™è¯¯çš„ç¬¦å· åè¾¹: æ­£ç¡®çš„ç¬¦å·
	nonvocal_conv_table = [('â€œâ€â€â€Ÿâ¹‚ââã€ã€ã€ŸÂ«Â»â ğŸ™·ğŸ™¶ğŸ™¸ï¼‚ã€Œã€â¸—	','"'),('â€¹â€ºâ€™â€˜â€›â›âœâŸâ€²Ê»Ê¾Â´`Ê¼Ê¿Ì•Ê½ÕÌ’Ì”ï¸Â´ï¼‡á ˆÌ’','\''),('â€’â€”â€“â€•â€‘â€ãƒ¼âˆ’â”€Â¬â€•â€”â€“â€ï¼â€‘-ï¹£âƒá †â€§â¹€â¸šã‚ ÖŠÍœ','-'),('â€¦â‹¯á ','...'),('â¸´Ì¦ØŒğŸ„Šâ¹âªâ¹Œâ¸²â€šê›µğ–º—ğª‡ğŸ„',','),('â„âˆ•Ì·â¼ƒ','/')]
	proc_lines = []
	for line in strlines:
		proc_line = ''
		for char in line:
			proc_char = char
			for pair in nonvocal_conv_table:
				if char in pair[0]:
					proc_char = pair[1]
				else:
					pass
			proc_line += proc_char
		proc_lines.append(proc_line)
	return proc_lines

# åˆ¤æ–­æ˜¯å¦å«æœ‰URL
def is_contain_url(token):
	url = '\.com|\.net|\.org|\.edu|\.gov|\.mil|\.aero|\.asia|\.biz|\.cat|\.coop|\.info|\.int|\.jobs|\.mobi|\.museum|\.name|\.post|\.pro|\.tel|\.travel|\.xxx|\.ac|\.ad|\.ae|\.af|\.ag|\.ai|\.al|\.am|\.an|\.ao|\.aq|\.ar|\.as|\.at|\.au|\.aw|\.ax|\.az|\.ba|\.bb|\.bd|\.be|\.bf|\.bg|\.bh|\.bi|\.bj|\.bm|\.bn|\.bo|\.br|\.bs|\.bt|\.bv|\.bw|\.by|\.bz|\.ca|\.cc|\.cd|\.cf|\.cg|\.ch|\.ci|\.ck|\.cl|\.cm|\.cn|\.co|\.cr|\.cs|\.cu|\.cv|\.cx|\.cy|\.cz|\.dd|\.de|\.dj|\.dk|\.dm|\.do|\.dz|\.ec|\.ee|\.eg|\.eh|\.er|\.es|\.et|\.eu|\.fi|\.fj|\.fk|\.fm|\.fo|\.fr|\.ga|\.gb|\.gd|\.ge|\.gf|\.gg|\.gh|\.gi|\.gl|\.gm|\.gn|\.gp|\.gq|\.gr|\.gs|\.gt|\.gu|\.gw|\.gy|\.hk|\.hm|\.hn|\.hr|\.ht|\.hu|\.id|\.ie|\.il|\.im|\.in|\.io|\.iq|\.ir|\.is|\.it|\.je|\.jm|\.jo|\.jp|\.ke|\.kg|\.kh|\.ki|\.km|\.kn|\.kp|\.kr|\.kw|\.ky|\.kz|\.la|\.lb|\.lc|\.li|\.lk|\.lr|\.ls|\.lt|\.lu|\.lv|\.ly|\.ma|\.mc|\.md|\.me|\.mg|\.mh|\.mk|\.ml|\.mm|\.mn|\.mo|\.mp|\.mq|\.mr|\.ms|\.mt|\.mu|\.mv|\.mw|\.mx|\.my|\.mz|\.na|\.nc|\.ne|\.nf|\.ng|\.ni|\.nl|\.no|\.np|\.nr|\.nu|\.nz|\.om|\.pa|\.pe|\.pf|\.pg|\.ph|\.pk|\.pl|\.pm|\.pn|\.pr|\.ps|\.pt|\.pw|\.py|\.qa|\.re|\.ro|\.rs|\.ru|\.rw|\.sa|\.sb|\.sc|\.sd|\.se|\.sg|\.sh|\.si|\.sj|\. Ja|\.sk|\.sl|\.sm|\.sn|\.so|\.sr|\.ss|\.st|\.su|\.sv|\.sx|\.sy|\.sz|\.tc|\.td|\.tf|\.tg|\.th|\.tj|\.tk|\.tl|\.tm|\.tn|\.to|\.tp|\.tr|\.tt|\.tv|\.tw|\.tz|\.ua|\.ug|\.uk|\.us|\.uy|\.uz|\.va|\.vc|\.ve|\.vg|\.vi|\.vn|\.vu|\.wf|\.ws|\.ye|\.yt|\.yu|\.za|\.zm|\.zw|\.php|\.html|\.htm|\.asp|http|https|ftp|www|://'
	if re.search(url,token) != None:
		res = True
	else:
		res = False
	return res

# æ˜¯å¦å«æœ‰è´§å¸ç¬¦å·
def is_contain_currency(token):
	regex = re.compile('[$Â£â‚¬Â¥â‚¹](\d+)?(,)?(\d+)?(\.?)(\d)+')
	if re.search(regex,token) != None:
		res = True
	else:
		res = False
	return res

def keep_only_currency(token):
	regex = re.compile('[$Â£â‚¬Â¥â‚¹](\d+)?(,)?(\d+)?(\.?)(\d)+')
	match = re.search(regex,token)
	res = match.group()
	return res

# æ›¿æ¢URLä¸º[URL]
def replace_url(token):
	if is_contain_url(token):
		res = '[URL]'
	return res

def lower_token(tklines):
	proc_lines = []
	sent_cnt = 0
	for line in tklines:
		sent_cnt += 1
		proc_line = []
		for token in line:
			proc_line.append(token.lower())
		proc_lines.append(proc_line)
	return proc_lines

# è½½å…¥lepzigè¯­æ–™åº“(sentences), å¤„ç†æˆåˆæ­¥çš„tokenized_lines
# è®¾è®¡æµç¨‹å›¾è§ corprep_pipelineæ€»ä½“è§„åˆ’_20211024171633.drawio
# åŒ…å«æ€»ä½“è§„åˆ’ä¸­çš„ æ–‡æ¡£å±‚ å’Œ å¥å­å±‚
def load_lepzig(filename):
	# æ–‡ä»¶å±‚: æ‰“å¼€æ–‡ä»¶
	print(f'{PRINT_TITLE}>>>load_lepzig: Loading Lepzig sentences...')
	with open(filename,mode='r',encoding='utf-8') as lpzfile:
		doc = lpzfile.read()
	print(f'{PRINT_TITLE}>>>load_lepzig: Document length: {len(doc)} chars')

	# æ–‡æ¡£å±‚: æ›¿æ¢ä¹±ç 
	print(f'{PRINT_TITLE}>>>load_lepzig: Replacing gibberish in whole doc, 1st time...')
	doc = replace_gibberish_doc(doc)
	print(f'{PRINT_TITLE}>>>load_lepzig: Replacing gibberish in whole doc, 2nd time...')
	doc = replace_gibberish_doc(doc)
	print(f'{PRINT_TITLE}>>>load_lepzig: Replacing gibberish in whole doc, 3rd time...')
	doc = replace_gibberish_doc(doc)
	print(f'{PRINT_TITLE}>>>load_lepzig: Document length after gibberish replaced: {len(doc)} chars')

	# æ–‡æ¡£å±‚: åˆ†è¡Œ
	print(f'{PRINT_TITLE}>>>load_lepzig: Splitting lines...')
	strlines = doc.split(sep='\n')
	
	# æ–‡æ¡£å±‚: åˆ é™¤åˆ†è¡Œé€ æˆçš„æœ€åä¸€è¡Œç©ºè¡Œ
	strlines = strlines[:-1]
	print(f'{PRINT_TITLE}>>>load_lepzig: Total strlines {len(strlines)} lines')

	# å¥å­å±‚: å»é™¤è¡Œå·
	print(f'{PRINT_TITLE}>>>load_lepzig: Revmoing line numbers...')
	strlines = [re.sub('^\d+\t','',line) for line in strlines]

	# å¥å­å±‚: æ›¿æ¢HTMLå­—ç¬¦å®ä½“
	print(f'{PRINT_TITLE}>>>load_lepzig: Replacing html entities...')
	strlines = replace_html_char_entity(strlines)
	print(f'{PRINT_TITLE}>>>load_lepzig: Total lines after HTML replaced: {len(strlines)} lines')

	# å¥å­å±‚: æ›¿æ¢ä¹±ç 
	print(f'{PRINT_TITLE}>>>load_lepzig: Replacing gibberish after HTML replaced, 1st time...')
	strlines = replace_gibberish_strlines(strlines)
	print(f'{PRINT_TITLE}>>>load_lepzig: Replacing gibberish after HTML replaced, 2nd time...')
	strlines = replace_gibberish_strlines(strlines)
	print(f'{PRINT_TITLE}>>>load_lepzig: Replacing gibberish after HTML replaced, 3rd time...')
	strlines = replace_gibberish_strlines(strlines)
	print(f'{PRINT_TITLE}>>>load_lepzig: Document length after gibberish replaced: {len(" ".join(strlines))} chars')

	# å¥å­å±‚: æ›¿æ¢éASCIIæ ‡ç‚¹ç¬¦å·
	print(f'{PRINT_TITLE}>>>load_lepzig: replace_nonascii_punct...')
	strlines = replace_nonascii_punct(strlines)

	# å¥å­å±‚: æŸ¥æ‰¾æ‹†åˆ†å¤šåˆä¸€è¡Œ
	# print(f'{PRINT_TITLE}>>>load_lepzig: Splitting duplex strlines...')
	# strlines = split_duplex_lines(strlines)
	# print(f'{PRINT_TITLE}>>>load_lepzig: Total strlines after splitting: {len(strlines)} ...')
	
	# å¥å­å±‚: åˆ é™¤å¤šä½™ç©ºæ ¼
	print(f'{PRINT_TITLE}>>>load_lepzig: Removing extra spaces...')
	strlines = remove_extra_space(strlines)
	
	# å¥å­å±‚: åˆ é™¤ç©ºè¡Œ
	print(f'{PRINT_TITLE}>>>load_lepzig: Removing empty strlines...')
	strlines = remove_empty_strlines(strlines)
	print(f'{PRINT_TITLE}>>>load_lepzig: Total Lines after Empty Lines Removed {len(strlines)}')

	# å¥å­å±‚: ç©ºæ ¼åˆ†è¯
	tklines = [line.split() for line in strlines]
	return tklines

# è®¾è®¡æµç¨‹å›¾è§ corprep_sanitizeæµç¨‹å›¾_20211018151342.drawio
# ä¸Šçº§è®¾è®¡æµç¨‹å›¾è§ corprep_pipelineæ€»ä½“è§„åˆ’_20211024171633.drawio
# åŒ…å«æ€»ä½“è§„åˆ’ä¸­çš„tokenå±‚
def sanitize_token(tklines):
	proc_lines = []
	for line in tklines:
		proc_line = []
		for token in line:
			# é•¿åº¦ä¸º0: æ˜¯
			if len(token) == 0:
				pass
			# é•¿åº¦ä¸º0: å¦
			else:
				# åŒ…å«éæ³•å­—ç¬¦: æ˜¯ -> ä¸å…¥åˆ—
				if is_contain_illegal_char(token) == True:
					pass
				# åŒ…å«éæ³•å­—ç¬¦: å¦
				else:
					# çº¯ç²¹token: æ˜¯
					if is_homo(token) == True:
						if is_only_legalabc(token) == True:
							proc_line.append(token)
						elif is_only_numeric(token) == True:
							proc_line.append(token)
						elif is_only_nonvocal(token) == True:
							continue
					# çº¯ç²¹token: å¦
					else:
						# å­—æ¯ + ç¬¦å·æ··åˆ: æ˜¯
						if is_mix_legalabc_nonvocal(token) == True:
							if is_contain_onlyone_legalabc_seq(token) == True:
								proc_line.append(keep_only_legalabc_seq(token))
							elif is_contain_sota(token):
								proc_line.append(keep_only_sota(token))
							elif is_usa(token):
								proc_line.append(strip_nonvocal_mix_nonvocal(token))
							elif is_abc_comma_abc(token):
								proc_line += split_abc_comma_abc(token)
							elif is_contain_url(token):
								proc_line.append(replace_url(token))
							elif is_abc_endpunct_abc(token):
								proc_line += split_abc_endpunct_abc(token)
							elif is_contain_contract(token):
								proc_line.append(keep_only_contract(token))
							else:
								proc_line.append(strip_nonvocal_mix_nonvocal(token))
						# å­—æ¯ + æ•°å­—æ··åˆ: æ˜¯
						elif is_mix_legalabc_num(token):
							proc_line.append(token)
						# æ•°å­— + ç¬¦å·æ··åˆ: æ˜¯
						elif is_mix_num_nonvocal(token):
							if is_contain_currency(token):
								proc_line.append(keep_only_currency(token))
							else:
								proc_line.append(strip_nonvocal_mix_nonvocal(token))
						# å­—æ¯ + æ•°å­— + ç¬¦å·æ··åˆ: æ˜¯
						elif is_mix_legalabc_num_nonvocal(token):
							# URL: æ˜¯
							if is_contain_url(token):
								proc_line.append(replace_url(token))
							# åŒ…æ‹¬ä¸”ä¸€æ®µä¸”ä»…ä¸€æ®µè¿ç»­å­—æ¯: æ˜¯
							elif is_contain_onlyone_legalabc_seq(token) == True:
								proc_line.append(keep_only_legalabc_seq(token))
							# éURL ä¹Ÿä¸åŒ…æ‹¬ä¸”ä¸€æ®µä¸”ä»…ä¸€æ®µè¿ç»­å­—æ¯
							else:
								proc_line.append(strip_nonvocal_mix_nonvocal(token))
						else:
							print(f'impossible token {token} detected, logical problem in sanitizer.')
		proc_lines.append(proc_line)
	return proc_lines

def stats(tklines):
	stat_log = []
	token_cnt = 0
	num_token_cnt = 0
	abc_token_cnt = 0
	abc_num_token_cnt = 0
	abc_nonvocal_token_cnt = 0
	num_nonvocal_token_cnt = 0
	abc_num_nonvocal_token_cnt = 0
	for line in tklines:
		for token in line:
			token_cnt += 1
			if is_only_legalabc(token) == True:
				abc_token_cnt += 1
			elif is_only_numeric(token) == True:
				num_token_cnt += 1
			elif is_mix_legalabc_num(token) == True:
				abc_num_token_cnt += 1
			elif is_mix_legalabc_nonvocal(token) == True:
				abc_nonvocal_token_cnt += 1
			elif is_mix_num_nonvocal(token) == True:
				num_nonvocal_token_cnt += 1
			else:
				abc_num_nonvocal_token_cnt += 1
	print(f'Stats: Total Tokens: \t{token_cnt}')
	stat_log.append(f'Stats: Total Tokens: \t{token_cnt}')
	print(f'Stats: Total Illegal Tokens: \t{len(illegal_tokens)}')
	stat_log.append(f'Stats: Total Illegal Tokens: \t{len(illegal_tokens)}')
	print(f'Stats: Total Lines after Processing: \t{len(tklines)}')
	stat_log.append(f'Stats: Total Lines after Processing: \t{len(tklines)}')
	print(f'Stats: Abc Tokens: \t{abc_token_cnt}({100*abc_token_cnt/token_cnt:.3f}%)')
	stat_log.append(f'Stats: Abc Tokens: \t{abc_token_cnt}({100*abc_token_cnt/token_cnt:.3f}%)')
	print(f'Stats: Abc + Num Tokens: \t{abc_num_token_cnt}({100*abc_num_token_cnt/token_cnt:.3f}%)')
	stat_log.append(f'Stats: Abc + Num Tokens: \t{abc_num_token_cnt}({100*abc_num_token_cnt/token_cnt:.3f}%)')
	print(f'Stats: Abc + Sym Tokens: \t{abc_nonvocal_token_cnt}({100*abc_nonvocal_token_cnt/token_cnt:.3f}%)')
	stat_log.append(f'Stats: Abc + Sym Tokens: \t{abc_nonvocal_token_cnt}({100*abc_nonvocal_token_cnt/token_cnt:.3f}%)')
	print(f'Stats: Abc + Num + Sym Tokens: \t{abc_num_nonvocal_token_cnt}({100*abc_num_nonvocal_token_cnt/token_cnt:.3f}%)')
	stat_log.append(f'Stats: Abc + Num + Sym Tokens: \t{abc_num_nonvocal_token_cnt}({100*abc_num_nonvocal_token_cnt/token_cnt:.3f}%)')
	print(f'Stats: Num Tokens: \t{num_token_cnt}({100*num_token_cnt/token_cnt:.3f}%)')
	stat_log.append(f'Stats: Num Tokens: \t{num_token_cnt}({100*num_token_cnt/token_cnt:.3f}%)')
	print(f'Stats: Num + Sym Tokens: \t{num_nonvocal_token_cnt}({100*num_nonvocal_token_cnt/token_cnt:.3f}%)')
	stat_log.append(f'Stats: Num + Sym Tokens: \t{num_nonvocal_token_cnt}({100*num_nonvocal_token_cnt/token_cnt:.3f}%)')
	return stat_log

# è®¾è®¡å›¾è§corprep_pipelineæ€»ä½“è§„åˆ’_20211024171633.drawio
def pipeline(filename,savepath):
	filenamepobj = Path(filename)
	savepathpobj = Path(savepath)
	lepzig_sents_filename = filenamepobj.name
	print(f'{PRINT_TITLE}: Processing file {filenamepobj.stem}')
	print(f"====================PROCESS({LANG})====================")
	# å¥å­å±‚
	print(f'{PRINT_TITLE}: load_lepzig...')
	proc_lines = load_lepzig(filename)
	# tokenå±‚
	print(f'{PRINT_TITLE}: log_illegal_token...')
	global illegal_tokens
	illegal_tokens = log_illegal_token(proc_lines)
	print(f'{PRINT_TITLE}: sanitize_token...')
	proc_lines = sanitize_token(proc_lines)
	print(f'{PRINT_TITLE}: lower_token...')
	proc_lines = lower_token(proc_lines)
	# æ”¶å°¾å±‚
	print(f'{PRINT_TITLE}: remove_empty_tklines')
	proc_lines = remove_empty_tklines(proc_lines)
	print(f"====================SUMMARY({LANG})====================")
	stat = stats(proc_lines)
	print(f"====================SAVING({LANG})====================")
	try:
		with open(f'{savepathpobj.joinpath(filenamepobj.stem)}.pkl',mode='wb') as save_pkl:
			pickle.dump(proc_lines,save_pkl)
			print(f'{PRINT_TITLE}: Result pickle saved at: {savepathpobj.joinpath(filenamepobj.stem)}.pkl')
	except:
		print('Failed saving result pickle file.')
	try:
		with open(f'{savepathpobj.joinpath(filenamepobj.stem)}_except.txt',mode='w',encoding='utf-8') as save_except:
			save_except.write(illegal_tokens)
			print(f'{PRINT_TITLE}: Illegal tokens saved at: {savepathpobj.joinpath(filenamepobj.stem)}.txt')
	except:
		with open(f'{savepathpobj.joinpath(filenamepobj.stem)}_except.pkl',mode='wb') as save_except_fail_pkl:
			pickle.dump(illegal_tokens,save_except_fail_pkl)
		print(f'Failed saving illegal tokens. PKL saved instead at {savepathpobj.joinpath(filenamepobj.stem)}_except.pkl')
	try:
		with open(f'{savepathpobj.joinpath(filenamepobj.stem)}_stats.txt',mode='w',encoding='utf-8') as save_stats:
			for i in stat:
				save_stats.write(i)
				save_stats.write('\n')
			print(f'{PRINT_TITLE}: Statistics saved at: {savepathpobj.joinpath(filenamepobj.stem)}.txt')
	except:
		print(f'Failed saving statistics.')
	try:
		with open(f'{savepathpobj.joinpath(filenamepobj.stem)}_pdoc.txt',mode='w',encoding='utf-8') as save_pdoc:
			for tkline in proc_lines:
				save_pdoc.write(' '.join(tkline))
				save_pdoc.write('\n')
			print(f'{PRINT_TITLE}: Processed docs saved at: {savepathpobj.joinpath(filenamepobj.stem)}.txt')
	except:

		print(f'Failed saving processed docs.')
	return proc_lines